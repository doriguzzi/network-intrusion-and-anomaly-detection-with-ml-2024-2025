{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ea4da9",
   "metadata": {},
   "source": [
    "# DDoS traffic classification\n",
    "In this laboratory, you will evaluate the capability of a pre-trained deep learning model to differentiate between DDoS traffic and benign traffic. The provided script loads CNN model trained before. The model is evaluated on then test set, with the classification accuracy being displayed in the notebook and saved in a CSV file.\n",
    "\n",
    "| <img src=\"ml-workflow.png\" width=\"60%\"> |\n",
    "|:--:|\n",
    "| Convolutional Neural Network (LUCID) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aec2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Course on Network Intrusion and Anomaly Detection with Deep Learning\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import h5py\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import logging\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import  f1_score, accuracy_score\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4f7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    filename = glob.glob(path)[0]\n",
    "    dataset = h5py.File(filename, \"r\")\n",
    "    set_x_orig = np.array(dataset[\"set_x\"][:])  # features\n",
    "    set_y_orig = np.array(dataset[\"set_y\"][:])  # labels\n",
    "\n",
    "    X_train = np.reshape(set_x_orig, (set_x_orig.shape[0], set_x_orig.shape[1], set_x_orig.shape[2], 1))\n",
    "    Y_train = set_y_orig\n",
    "\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6644f26",
   "metadata": {},
   "source": [
    "## Performance metrics\n",
    "The following method computes the metrics to assess the performance of the models on the given datasets. Both accuracy and F1 Score are widely used metrics in many domains of the computer science. More information can be found in the ```sklearn``` documentation ([accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score), [F1 Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)) and in online documentation (e.g., [Metrics to Evaluate your Machine Learning Algorithm](https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27630a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(Y_true, Y_pred):\n",
    "    Y_true = Y_true.reshape((Y_true.shape[0], 1))\n",
    "    accuracy = accuracy_score(Y_true, Y_pred)\n",
    "    f1 = f1_score(Y_true, Y_pred)\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = glob.glob(\"./output/10t-10n*.h5\") # list of pre-trained models\n",
    "print (\"Model List: \",model_list)\n",
    "dataset_filelist = glob.glob(\"./sample-dataset/*test.hdf5\") # list of test sets\n",
    "print (\"Test sets List: \",dataset_filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_fieldnames = ['Dataset', 'Samples', 'Time', 'Accuracy', 'F1Score', 'Model']\n",
    "predict_file = open('./results.csv', 'a', newline='')\n",
    "predict_file.truncate(0)  # clean the file content (as we open the file in append mode)\n",
    "predict_writer = csv.DictWriter(predict_file, fieldnames=classify_fieldnames)\n",
    "predict_writer.writeheader()\n",
    "predict_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6282f6f",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "In the last cell, the script evaluates all the models on all the test sets available in the list.  The pre-trained models are loaded from the file system using the ```load_model``` method from Keras. The ```load_dataset``` method, previously defined, retrieves the test set and returns two numpy arrays: ```X```, containing the test examples, and ```Y```, providing the labels. \n",
    "\n",
    "The ```model.predict``` method is used to compute predictions for the entire test set in batches of 2048 examples. Note that adjusting this value will affect the prediction time, but not the accuracy scores. Lastly, the ```compute_metric``` method is utilized to calculate the accuracy and F1 score metrics, which are then used to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8349a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_path in model_list:\n",
    "    model = load_model(model_path)\n",
    "    print (model.summary())\n",
    "    model_filename = model_path.split('/')[-1].split('.')[0]\n",
    "    for dataset_file in dataset_filelist:\n",
    "        dataset_filename = dataset_file.split('/')[-1].split('.')[0]\n",
    "        X, Y = load_dataset(dataset_file)\n",
    "        Y_true = Y\n",
    "        pt0 = time.time()\n",
    "        Y_pred = np.squeeze(model.predict(X, batch_size=2048) > 0.5)\n",
    "        pt1 = time.time()\n",
    "        accuracy, f1 = compute_metrics(Y_true,Y_pred)\n",
    "        row = {'Dataset': dataset_filename, 'Samples': Y_true.shape[0], 'Time': '{:10.3f}'.format(pt1-pt0), 'Accuracy': accuracy,\n",
    "               'F1Score': f1, 'Model': model_filename}\n",
    "        print (row)\n",
    "        predict_writer.writerow(row)\n",
    "predict_file.close()\n",
    "print(\"Classification results saved in file: \", predict_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26a1f08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
